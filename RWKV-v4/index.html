<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<title></title>
	<script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
</head>
<body>
	<script type="text/javascript">
		// use an async context to call onnxruntime functions.
		async function main() {
			try {
				ort.env.logLevel = "verbose"
				ort.env.logLevelInternal = "verbose"

				const sessionOption = {
					executionProviders: ['webgl'],
					graphOptimizationLevel: 'all'
				}

				// create a new session and load the specific model.
				const session = await ort.InferenceSession.create('./matmul.onnx', sessionOption)

				const start = performance.now()

				const mat_d = new Float32Array(768*768)
				const vec_d = new Float32Array(768)

				mat_d.fill(1)
				vec_d.fill(5)

				const mat = new ort.Tensor('float32', mat_d, [768, 768])
				const vec = new ort.Tensor('float32', vec_d, [768, 1])

				// prepare feeds. use model input names as keys.
				var feeds = { mat: mat, vec: vec }

				// feed inputs and run
				for (var i = 0; i < 64; i++) {
					var results = await session.run(feeds)
					console.log(results.x.data)
					console.log(1 + i, "/", 64)
				}

				const stop = performance.now()

				console.log(((stop - start)/64) + "ms/token")
			} catch (e) {
				console.log(`failed to inference ONNX model: ${e}.`)
			}
		}

		console.log("Hello")
		main()
	</script>
</body>
</html>
